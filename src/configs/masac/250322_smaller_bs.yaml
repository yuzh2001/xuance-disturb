agent: "MATD3"
env_name: "MultiWalker"
env_id: "multiwalker_v9"
env_seed: 1
continuous_action: True
learner: "MASAC_Learner"
policy: "Gaussian_MASAC_Policy"
representation: "Basic_Identical"
vectorize: "SubprocVecMultiAgentEnv"
runner: "MARL"

# more
benchmark: True
render: False
distributed_training: False
dl_toolbox: "torch"  # The deep learning toolbox. Choices: "torch", "mindspore", "tensorflow"

# logging
logger: "wandb"  # Choices: "tensorboard", "wandb".
wandb_user_name: "yuzh2001-iscas"
project_name: "XuanCe_Benchmark"

render_mode: 'rgb_array' # Choices: 'human', 'rgb_array'.
fps: 50  # The frames per second for the rendering videos in log file.
test_mode: False  # Whether to run in test mode.
device: "cuda:0"  # Choose an calculating device. PyTorch: "cpu", "cuda:0"; TensorFlow: "cpu"/"CPU", "gpu"/"GPU"; MindSpore: "CPU", "GPU", "Ascend", "Davinci".
master_port: '12355'  # The master port for current experiment when use distributed training.

# environment settings
n_walkers: 3  # walker数量
position_noise: 1e-3  # 位置观测噪声
angle_noise: 1e-3  # 角度观测噪声
forward_reward: 1.0  # 前进奖励系数
terminate_reward: -100.0  # 终止惩罚
fall_reward: -10.0  # 跌倒惩罚
shared_reward: True  # 是否共享奖励
terminate_on_fall: True  # 是否在跌倒时终止
remove_on_fall: False  # 是否在跌倒时移除walker
terrain_length: 200  # 地形长度
max_cycles: 500  # 最大步数

# network architecture

representation_hidden_size: []  # the units for each hidden layer
actor_hidden_size: [128, 128]
critic_hidden_size: [128, 128]
activation: 'relu'
activation_action: 'sigmoid'
use_parameter_sharing: True
use_actions_mask: False

# recurrent settings for Basic_RNN representation.
use_rnn: False  # If to use recurrent neural network as representation. (The representation should be "Basic_RNN").
rnn: "GRU"  # The type of recurrent layer.
fc_hidden_sizes: [64, 64, 64]  # The hidden size of feed forward layer in RNN representation.
recurrent_hidden_size: 64  # The hidden size of the recurrent layer.
N_recurrent_layers: 1  # The number of recurrent layer.
dropout: 0  # dropout should be a number in range [0, 1], the probability of an element being zeroed.
normalize: "LayerNorm"  # Layer normalization.
initialize: "orthogonal"  # Network initializer.
gain: 0.01


# training parameters
seed: 1  # 随机种子
parallels: 16  # 并行环境数量
buffer_size: 1000000  # 增大buffer size以存储更多经验
batch_size: 1024
learning_rate_actor: 0.0005  # learning rate for actor
learning_rate_critic: 0.0005  # learning rate for critic
gamma: 0.99  # discount factor
tau: 0.005  # soft update for target networks
alpha: 0.01
use_automatic_entropy_tuning: True

start_training: 10  # start training after n episodes
running_steps: 15000000
training_frequency: 1

use_grad_clip: False
grad_clip_norm: 10

# training and evaluation
eval_interval: 50000  # 评估间隔
test_episode: 20  # 测试轮数

# logging
log_dir: "./logs/matd3/"  # 日志保存路径
model_dir: "./models/matd3/"  # 模型保存路径